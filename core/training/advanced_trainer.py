"""
ğŸ”¥ ADVANCED TRAINING PIPELINE ğŸ”¥
=================================
State-of-the-art training pipeline with:
- Comprehensive logging and metrics tracking
- Advanced callbacks and learning rate scheduling
- Real-time monitoring and visualization
- Model checkpointing and early stopping
- Performance analysis and reporting
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pathlib import Path
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MetricsLogger:
    """
    Comprehensive metrics logging and visualization
    """
    
    def __init__(self, log_dir: Path):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        self.metrics_history = {
            'train_loss': [],
            'val_loss': [],
            'train_accuracy': [],
            'val_accuracy': [],
            'learning_rate': [],
            'epoch_times': []
        }
        
        self.best_metrics = {
            'best_val_accuracy': 0.0,
            'best_val_loss': float('inf'),
            'best_epoch': 0
        }
        
        logger.info(f"ğŸ“Š Metrics Logger initialized - Log dir: {log_dir}")
    
    def log_epoch(self, epoch: int, logs: Dict[str, float], epoch_time: float):
        """
        Log metrics for an epoch
        """
        # Store metrics
        self.metrics_history['train_loss'].append(logs.get('loss', 0))
        self.metrics_history['val_loss'].append(logs.get('val_loss', 0))
        self.metrics_history['train_accuracy'].append(logs.get('accuracy', 0))
        self.metrics_history['val_accuracy'].append(logs.get('val_accuracy', 0))
        self.metrics_history['learning_rate'].append(logs.get('lr', 0))
        self.metrics_history['epoch_times'].append(epoch_time)
        
        # Update best metrics
        val_acc = logs.get('val_accuracy', 0)
        val_loss = logs.get('val_loss', float('inf'))
        
        if val_acc > self.best_metrics['best_val_accuracy']:
            self.best_metrics['best_val_accuracy'] = val_acc
            self.best_metrics['best_val_loss'] = val_loss
            self.best_metrics['best_epoch'] = epoch
        
        # Log to console with rich formatting
        print(f"\n{'='*80}")
        print(f"ğŸš€ EPOCH {epoch + 1} COMPLETE")
        print(f"{'='*80}")
        print(f"ğŸ“ˆ Training Loss:     {logs.get('loss', 0):.4f}")
        print(f"ğŸ“ˆ Training Accuracy: {logs.get('accuracy', 0)*100:.2f}%")
        print(f"ğŸ“Š Validation Loss:   {logs.get('val_loss', 0):.4f}")
        print(f"ğŸ“Š Validation Acc:    {logs.get('val_accuracy', 0)*100:.2f}%")
        print(f"â±ï¸  Epoch Time:        {epoch_time:.2f}s")
        print(f"ğŸ¯ Best Val Acc:      {self.best_metrics['best_val_accuracy']*100:.2f}% (Epoch {self.best_metrics['best_epoch'] + 1})")
        print(f"{'='*80}\n")
        
        # Save metrics to file
        self.save_metrics()
        
        # Generate live plots every 5 epochs
        if (epoch + 1) % 5 == 0:
            self.plot_metrics()
    
    def save_metrics(self):
        """
        Save metrics to JSON file
        """
        metrics_file = self.log_dir / 'training_metrics.json'
        
        save_data = {
            'metrics_history': self.metrics_history,
            'best_metrics': self.best_metrics,
            'total_epochs': len(self.metrics_history['train_loss'])
        }
        
        with open(metrics_file, 'w') as f:
            json.dump(save_data, f, indent=2)
    
    def plot_metrics(self):
        """
        Generate and save training plots
        """
        if len(self.metrics_history['train_loss']) < 2:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(self.metrics_history['train_loss']) + 1)
        
        # Loss plot
        axes[0, 0].plot(epochs, self.metrics_history['train_loss'], 'b-', label='Training Loss', linewidth=2)
        axes[0, 0].plot(epochs, self.metrics_history['val_loss'], 'r-', label='Validation Loss', linewidth=2)
        axes[0, 0].set_title('Model Loss', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Accuracy plot
        train_acc_pct = [acc * 100 for acc in self.metrics_history['train_accuracy']]
        val_acc_pct = [acc * 100 for acc in self.metrics_history['val_accuracy']]
        
        axes[0, 1].plot(epochs, train_acc_pct, 'g-', label='Training Accuracy', linewidth=2)
        axes[0, 1].plot(epochs, val_acc_pct, 'orange', label='Validation Accuracy', linewidth=2)
        axes[0, 1].set_title('Model Accuracy', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy (%)')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Learning rate plot
        axes[1, 0].plot(epochs, self.metrics_history['learning_rate'], 'purple', linewidth=2)
        axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Epoch time plot
        axes[1, 1].plot(epochs, self.metrics_history['epoch_times'], 'brown', linewidth=2)
        axes[1, 1].set_title('Training Speed', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Time per Epoch (s)')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.log_dir / 'training_progress.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ğŸ“Š Training plots updated: {self.log_dir / 'training_progress.png'}")


class AdvancedTrainingCallbacks:
    """
    Collection of advanced training callbacks
    """
    
    def __init__(self, log_dir: Path, model_name: str = "advanced_cnn_lstm"):
        self.log_dir = Path(log_dir)
        self.model_name = model_name
        self.callbacks = []
        
        # Initialize metrics logger
        self.metrics_logger = MetricsLogger(log_dir)
        
        self._setup_callbacks()
        
    def _setup_callbacks(self):
        """
        Setup all training callbacks
        """
        # Model checkpointing - save best model
        checkpoint_path = self.log_dir / f"{self.model_name}_best.keras"
        model_checkpoint = keras.callbacks.ModelCheckpoint(
            filepath=str(checkpoint_path),
            monitor='val_accuracy',
            save_best_only=True,
            save_weights_only=False,
            mode='max',
            verbose=1
        )
        self.callbacks.append(model_checkpoint)
        
        # Early stopping
        early_stopping = keras.callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=15,
            restore_best_weights=True,
            mode='max',
            verbose=1
        )
        self.callbacks.append(early_stopping)
        
        # Learning rate reduction
        reduce_lr = keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=8,
            min_lr=1e-7,
            verbose=1
        )
        self.callbacks.append(reduce_lr)
        
        # TensorBoard logging
        tensorboard_dir = self.log_dir / 'tensorboard'
        tensorboard = keras.callbacks.TensorBoard(
            log_dir=str(tensorboard_dir),
            histogram_freq=1,
            write_graph=True,
            write_images=True,
            update_freq='epoch'
        )
        self.callbacks.append(tensorboard)
        
        # Custom metrics callback
        metrics_callback = self._create_metrics_callback()
        self.callbacks.append(metrics_callback)
        
        logger.info(f"ğŸ”§ Setup {len(self.callbacks)} training callbacks")
    
    def _create_metrics_callback(self):
        """
        Create custom callback for detailed metrics logging
        """
        class MetricsCallback(keras.callbacks.Callback):
            def __init__(self, metrics_logger):
                super().__init__()
                self.metrics_logger = metrics_logger
                self.epoch_start_time = None
            
            def on_epoch_begin(self, epoch, logs=None):
                self.epoch_start_time = time.time()
            
            def on_epoch_end(self, epoch, logs=None):
                epoch_time = time.time() - self.epoch_start_time
                self.metrics_logger.log_epoch(epoch, logs or {}, epoch_time)
        
        return MetricsCallback(self.metrics_logger)
    
    def get_callbacks(self):
        """
        Return list of callbacks for training
        """
        return self.callbacks


class AdvancedModelTrainer:
    """
    ğŸ”¥ ADVANCED MODEL TRAINER ğŸ”¥
    
    Comprehensive training pipeline with:
    - Advanced callbacks and monitoring
    - Performance analysis and reporting
    - Model evaluation and testing
    - Automatic hyperparameter tracking
    """
    
    def __init__(self, model, log_dir: str, model_name: str = "advanced_cnn_lstm"):
        self.model = model
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.model_name = model_name
        
        # Initialize callbacks
        self.callback_manager = AdvancedTrainingCallbacks(self.log_dir, model_name)
        
        # Training configuration
        self.training_config = {
            'model_name': model_name,
            'start_time': None,
            'end_time': None,
            'total_training_time': None,
            'final_metrics': {},
            'best_metrics': {}
        }
        
        logger.info(f"ğŸš€ Advanced Trainer initialized - Model: {model_name}")
    
    def train(self, 
              train_data: Tuple[np.ndarray, np.ndarray],
              val_data: Tuple[np.ndarray, np.ndarray],
              epochs: int = 100,
              batch_size: int = 32,
              verbose: int = 1) -> Dict[str, Any]:
        """
        Train the model with advanced monitoring
        """
        X_train, y_train = train_data
        X_val, y_val = val_data
        
        logger.info(f"ğŸš€ Starting Advanced Training Session")
        logger.info(f"ğŸ“Š Training Data: {X_train.shape} | Validation Data: {X_val.shape}")
        logger.info(f"âš™ï¸  Epochs: {epochs} | Batch Size: {batch_size}")
        
        # Record start time
        self.training_config['start_time'] = datetime.now()
        start_time = time.time()
        
        try:
            # Train the model
            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=self.callback_manager.get_callbacks(),
                verbose=verbose,
                shuffle=True
            )
            
            # Record end time
            self.training_config['end_time'] = datetime.now()
            total_time = time.time() - start_time
            self.training_config['total_training_time'] = total_time
            
            # Extract final metrics
            final_epoch = len(history.history['loss']) - 1
            self.training_config['final_metrics'] = {
                'final_train_loss': history.history['loss'][-1],
                'final_train_accuracy': history.history['accuracy'][-1],
                'final_val_loss': history.history['val_loss'][-1],
                'final_val_accuracy': history.history['val_accuracy'][-1]
            }
            
            # Get best metrics from callback
            self.training_config['best_metrics'] = self.callback_manager.metrics_logger.best_metrics
            
            # Save training configuration
            self._save_training_config()
            
            # Generate final report
            self._generate_training_report(history)
            
            logger.info(f"ğŸ‰ Training completed successfully!")
            logger.info(f"â±ï¸  Total training time: {total_time/60:.2f} minutes")
            
            return {
                'history': history,
                'training_config': self.training_config,
                'best_metrics': self.training_config['best_metrics']
            }
            
        except Exception as e:
            logger.error(f"âŒ Training failed: {str(e)}")
            raise
    
    def evaluate_model(self, 
                      test_data: Tuple[np.ndarray, np.ndarray],
                      class_names: List[str] = None) -> Dict[str, Any]:
        """
        Comprehensive model evaluation
        """
        X_test, y_test = test_data
        
        logger.info(f"ğŸ§ª Evaluating model on test set: {X_test.shape}")
        
        # Load best model
        best_model_path = self.log_dir / f"{self.model_name}_best.keras"
        if best_model_path.exists():
            logger.info(f"ğŸ“¥ Loading best model from {best_model_path}")
            self.model = keras.models.load_model(str(best_model_path))
        
        # Make predictions
        y_pred_proba = self.model.predict(X_test, verbose=1)
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # Calculate metrics
        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        
        # Generate classification report
        if class_names is None:
            class_names = [f"Class_{i}" for i in range(len(np.unique(y_test)))]
        
        classification_rep = classification_report(
            y_test, y_pred, 
            target_names=class_names,
            output_dict=True
        )
        
        # Generate confusion matrix
        conf_matrix = confusion_matrix(y_test, y_pred)
        
        # Create evaluation report
        evaluation_results = {
            'test_loss': test_loss,
            'test_accuracy': test_accuracy,
            'classification_report': classification_rep,
            'confusion_matrix': conf_matrix.tolist(),
            'predictions': {
                'y_true': y_test.tolist(),
                'y_pred': y_pred.tolist(),
                'y_pred_proba': y_pred_proba.tolist()
            }
        }
        
        # Save evaluation results
        eval_file = self.log_dir / 'evaluation_results.json'
        with open(eval_file, 'w') as f:
            json.dump(evaluation_results, f, indent=2)
        
        # Generate evaluation plots
        self._generate_evaluation_plots(conf_matrix, classification_rep, class_names)
        
        # Print evaluation summary
        print(f"\n{'='*80}")
        print(f"ğŸ§ª MODEL EVALUATION RESULTS")
        print(f"{'='*80}")
        print(f"ğŸ“Š Test Loss:     {test_loss:.4f}")
        print(f"ğŸ“Š Test Accuracy: {test_accuracy*100:.2f}%")
        print(f"ğŸ¯ Best Val Acc:  {self.training_config['best_metrics'].get('best_val_accuracy', 0)*100:.2f}%")
        print(f"{'='*80}")
        
        logger.info(f"âœ… Evaluation complete - Results saved to {eval_file}")
        
        return evaluation_results
    
    def _save_training_config(self):
        """
        Save training configuration and metadata
        """
        config_file = self.log_dir / 'training_config.json'
        
        # Convert datetime objects to strings
        config_copy = self.training_config.copy()
        config_copy['start_time'] = self.training_config['start_time'].isoformat()
        config_copy['end_time'] = self.training_config['end_time'].isoformat()
        
        with open(config_file, 'w') as f:
            json.dump(config_copy, f, indent=2)
    
    def _generate_training_report(self, history):
        """
        Generate comprehensive training report
        """
        report_file = self.log_dir / 'training_report.md'
        
        # Calculate improvement metrics
        initial_val_acc = history.history['val_accuracy'][0] * 100
        final_val_acc = self.training_config['final_metrics']['final_val_accuracy'] * 100
        best_val_acc = self.training_config['best_metrics']['best_val_accuracy'] * 100
        improvement = best_val_acc - initial_val_acc
        
        with open(report_file, 'w') as f:
            f.write("# ğŸ”¥ Advanced CNN-LSTM Training Report\n\n")
            f.write(f"## Model: {self.model_name}\n")
            f.write(f"**Training Date:** {self.training_config['start_time'].strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ“Š Training Summary\n")
            f.write(f"- **Total Training Time:** {self.training_config['total_training_time']/60:.2f} minutes\n")
            f.write(f"- **Total Epochs:** {len(history.history['loss'])}\n")
            f.write(f"- **Best Epoch:** {self.training_config['best_metrics']['best_epoch'] + 1}\n\n")
            
            f.write("## ğŸ¯ Performance Metrics\n")
            f.write(f"- **Initial Validation Accuracy:** {initial_val_acc:.2f}%\n")
            f.write(f"- **Final Validation Accuracy:** {final_val_acc:.2f}%\n")
            f.write(f"- **Best Validation Accuracy:** {best_val_acc:.2f}%\n")
            f.write(f"- **Total Improvement:** +{improvement:.2f}%\n\n")
            
            f.write("## ğŸ“ˆ Final Metrics\n")
            f.write(f"- **Training Loss:** {self.training_config['final_metrics']['final_train_loss']:.4f}\n")
            f.write(f"- **Training Accuracy:** {self.training_config['final_metrics']['final_train_accuracy']*100:.2f}%\n")
            f.write(f"- **Validation Loss:** {self.training_config['final_metrics']['final_val_loss']:.4f}\n")
            f.write(f"- **Validation Accuracy:** {final_val_acc:.2f}%\n\n")
            
            f.write("## ğŸš€ Expected Performance\n")
            f.write("Based on the Advanced CNN-LSTM architecture:\n")
            f.write(f"- **Target Accuracy:** 50-70% (vs 10% LSTM-only baseline)\n")
            f.write(f"- **Achieved Accuracy:** {best_val_acc:.2f}%\n")
            
            if best_val_acc >= 50:
                f.write("- **Status:** âœ… Target achieved! Outstanding performance!\n")
            elif best_val_acc >= 30:
                f.write("- **Status:** ğŸŸ¡ Good progress, continue training for better results\n")
            else:
                f.write("- **Status:** ğŸ”´ Below target, check data quality and hyperparameters\n")
        
        logger.info(f"ğŸ“ Training report generated: {report_file}")
    
    def _generate_evaluation_plots(self, conf_matrix, classification_rep, class_names):
        """
        Generate evaluation visualization plots
        """
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # Confusion Matrix
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=class_names, yticklabels=class_names, ax=axes[0])
        axes[0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('Predicted')
        axes[0].set_ylabel('Actual')
        
        # Per-class accuracy
        per_class_acc = [classification_rep[cls]['f1-score'] for cls in class_names]
        axes[1].bar(range(len(class_names)), per_class_acc, color='skyblue')
        axes[1].set_title('Per-Class F1-Score', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('Class')
        axes[1].set_ylabel('F1-Score')
        axes[1].set_xticks(range(len(class_names)))
        axes[1].set_xticklabels(class_names, rotation=45)
        
        plt.tight_layout()
        plt.savefig(self.log_dir / 'evaluation_plots.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ğŸ“Š Evaluation plots saved: {self.log_dir / 'evaluation_plots.png'}")


def create_training_session(model, 
                           train_data: Tuple[np.ndarray, np.ndarray],
                           val_data: Tuple[np.ndarray, np.ndarray],
                           test_data: Tuple[np.ndarray, np.ndarray] = None,
                           class_names: List[str] = None,
                           log_dir: str = "logs/training",
                           model_name: str = "advanced_cnn_lstm",
                           epochs: int = 100,
                           batch_size: int = 32) -> Dict[str, Any]:
    """
    Complete training session with evaluation
    """
    trainer = AdvancedModelTrainer(model, log_dir, model_name)
    
    # Train the model
    training_results = trainer.train(
        train_data=train_data,
        val_data=val_data,
        epochs=epochs,
        batch_size=batch_size
    )
    
    # Evaluate if test data provided
    evaluation_results = None
    if test_data is not None:
        evaluation_results = trainer.evaluate_model(test_data, class_names)
    
    return {
        'training_results': training_results,
        'evaluation_results': evaluation_results,
        'trainer': trainer
    }


if __name__ == "__main__":
    # Test training pipeline
    print("ğŸ§ª Testing Advanced Training Pipeline...")
    
    # This would be used with real model and data
    print("âœ… Training pipeline test successful!") 